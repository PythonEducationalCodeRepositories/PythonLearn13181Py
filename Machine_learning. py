import chromadb
import csv
import time
from chromadb.utils import embedding_functions  # Import ChromaDB's embedding functions

# Initialize ChromaDB client (persistent storage)
chroma_client = chromadb.PersistentClient(path="./chroma_db")

# Create collection in ChromaDB
collection = chroma_client.get_or_create_collection(name="incident_data")

# Initialize ChromaDB's default embedding function
default_ef = embedding_functions.DefaultEmbeddingFunction()

def add_question_answer_to_chromadb(data, chunk_number):
    """Processes and inserts data into ChromaDB while ensuring correct headers."""
    documents = []
    metadatas = []
    ids = []
    embeddings = []

    for idx, row in enumerate(data):
        # Ensure the row has the correct number of columns (Expected: 13)
        if len(row) < 13:
            print(f"âš ï¸ Skipping Row {idx + 1}: Expected 13 columns, but got {len(row)} â†’ {row}")
            continue  # Skip malformed rows

        # Extract columns based on new CSV headers
        site = row[0]  # At what site did this occur?
        company = row[1]  # At what company did this occur?
        sector = row[2]  # Sector
        ehs_business_unit = row[3]  # EH&S Business Unit
        ehs_sub_business_unit = row[4]  # EH&S Sub-Business Unit
        site_location = row[5]  # Site Location
        what_happened = row[6]  # What Happened?
        what_caused_the_incident = row[7]  # What do you think caused this incident?
        action_taken = row[8]  # What action(s) did you take?
        action_suggested = row[9]  # What action(s) do you suggest?
        t_mrc = row[10]  # T_MRC
        t_legal_entity = row[11]  # T_LEGAL_ENTITY
        type_short = row[12]  # Type

        # Generate unique ID using chunk number + row index
        unique_id = f"chunk{chunk_number}_row{idx}"

        # Prepare document text for embedding
        document_text = f"{what_happened} {what_caused_the_incident} {action_taken} {action_suggested}"

        # Generate embeddings using ChromaDB's default function
        embedding = default_ef([document_text])[0]  # Generate single embedding

        # Prepare metadata with corrected headers
        metadata = {
            "site": site,
            "company": company,
            "sector": sector,
            "ehs_business_unit": ehs_business_unit,
            "ehs_sub_business_unit": ehs_sub_business_unit,
            "site_location": site_location,
            "what_happened": what_happened,
            "what_caused_the_incident": what_caused_the_incident,
            "action_taken": action_taken,
            "action_suggested": action_suggested,
            "T_MRC": t_mrc,  # Corrected field name
            "T_LEGAL_ENTITY": t_legal_entity,  # Corrected field name
            "Type_short": type_short,  # Corrected field name
        }

        # Append to batch lists
        documents.append(document_text)
        metadatas.append(metadata)
        ids.append(unique_id)  # Use unique ID
        embeddings.append(embedding)

    # Insert into ChromaDB
    if documents:
        collection.add(ids=ids, embeddings=embeddings, documents=documents, metadatas=metadatas)
        print(f"âœ… Chunk {chunk_number}: Inserted {len(documents)} valid records into ChromaDB.")
    else:
        print(f"âš ï¸ Chunk {chunk_number}: No valid records found, skipping insertion.")

def chunk_list(lst, chunk_size):
    """Splits the data into smaller chunks"""
    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]

def process_chunk(chunk, chunk_number):
    """Processes and inserts each chunk into ChromaDB"""
    print(f"ðŸ”„ Processing Chunk {chunk_number} with {len(chunk)} records...")
    add_question_answer_to_chromadb(chunk, chunk_number)
    print(f"âœ… Completed Chunk {chunk_number}.")
    time.sleep(1)

# Read CSV and process data
data = []
with open('GoodSavesEntireDataset.csv', mode='r', encoding='utf-8-sig') as file:
    csv_reader = csv.reader(file)
    header = next(csv_reader)  # Skip header

    for row_num, row in enumerate(csv_reader, start=1):
        if len(row) < 13:  # Ensure it has enough columns
            print(f"âš ï¸ Warning: Row {row_num} has {len(row)} columns instead of 13. Skipping.")
            continue  # Skip malformed rows

        data.append(tuple(row))  # Store only valid rows

if data:
    chunk_size = 1000
    chunks = chunk_list(data, chunk_size)

    for chunk_number, chunk in enumerate(chunks, start=1):
        process_chunk(chunk, chunk_number)

    print(f"ðŸŽ‰ All {len(chunks)} chunks processed successfully. Total valid records inserted: {len(data)}")
else:
    print("âš ï¸ No valid data found in CSV file. Nothing to insert.")