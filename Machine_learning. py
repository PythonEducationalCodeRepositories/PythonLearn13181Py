import chromadb
import csv
import time
from chromadb.utils import embedding_functions  # Import ChromaDB's embedding functions

# Initialize ChromaDB client (persistent storage)
chroma_client = chromadb.PersistentClient(path="./chroma_db")

# Create collection in ChromaDB
collection = chroma_client.get_or_create_collection(name="incident_data")

# Initialize ChromaDB's default embedding function
default_ef = embedding_functions.DefaultEmbeddingFunction()

def add_question_answer_to_chromadb(data, chunk_number):
    """Processes and inserts data into ChromaDB using its default embedding function."""
    documents = []
    metadatas = []
    ids = []
    embeddings = []

    for idx, row in enumerate(data):
        # Ensure the row has the correct number of columns (Expected: 13)
        if len(row) < 13:
            print(f"âš ï¸ Skipping Row {idx + 1}: Expected 13 columns, but got {len(row)} â†’ {row}")
            continue  # Skip malformed rows

        # Extract columns safely
        site_location, sector, ehs_business_unit, ehs_sub_business_unit, site, company, what_happened, \
        what_caused_the_incident, action_taken, action_suggested, legal_entity, incident_type, mrc = row

        # Generate unique ID using chunk number + row index
        unique_id = f"chunk{chunk_number}_row{idx}"

        # Prepare document text for embedding
        document_text = f"{what_happened} {what_caused_the_incident} {action_taken} {action_suggested}"

        # Generate embeddings using ChromaDB's default function
        embedding = default_ef([document_text])[0]  # Generate single embedding

        # Prepare metadata
        metadata = {
            "site_location": site_location,
            "sector": sector,
            "ehs_business_unit": ehs_business_unit,
            "ehs_sub_business_unit": ehs_sub_business_unit,
            "site": site,
            "company": company,
            "legal_entity": legal_entity,
            "type": incident_type,
            "mrc": mrc,
        }

        # Append to batch lists
        documents.append(document_text)
        metadatas.append(metadata)
        ids.append(unique_id)  # Use unique ID
        embeddings.append(embedding)

    # Insert into ChromaDB
    if documents:
        collection.add(ids=ids, embeddings=embeddings, documents=documents, metadatas=metadatas)
        print(f"âœ… Chunk {chunk_number}: Inserted {len(documents)} valid records into ChromaDB.")
    else:
        print(f"âš ï¸ Chunk {chunk_number}: No valid records found, skipping insertion.")

def chunk_list(lst, chunk_size):
    """Splits the data into smaller chunks"""
    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]

def process_chunk(chunk, chunk_number):
    """Processes and inserts each chunk into ChromaDB"""
    print(f"ðŸ”„ Processing Chunk {chunk_number} with {len(chunk)} records...")
    add_question_answer_to_chromadb(chunk, chunk_number)
    print(f"âœ… Completed Chunk {chunk_number}.")
    time.sleep(1)

# Read CSV and process data
data = []
with open('GoodSavesEntireDataset.csv', mode='r', encoding='utf-8-sig') as file:
    csv_reader = csv.reader(file)
    header = next(csv_reader)  # Skip header

    for row_num, row in enumerate(csv_reader, start=1):
        if len(row) < 13:  # Ensure it has enough columns
            print(f"âš ï¸ Warning: Row {row_num} has {len(row)} columns instead of 13. Skipping.")
            continue  # Skip malformed rows

        data.append(tuple(row))  # Store only valid rows

if data:
    chunk_size = 1000
    chunks = chunk_list(data, chunk_size)

    for chunk_number, chunk in enumerate(chunks, start=1):
        process_chunk(chunk, chunk_number)

    print(f"ðŸŽ‰ All {len(chunks)} chunks processed successfully. Total valid records inserted: {len(data)}")
else:
    print("âš ï¸ No valid data found in CSV file. Nothing to insert.")