import streamlit as st
from typing import TypedDict, List, Union
import tempfile
import os
import httpx
import tiktoken

# LangChain imports
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langgraph.graph import StateGraph, START, END
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains.retrieval_qa.base import RetrievalQA

# PDF processing
from pdfplumber.high_level import extract_text

# Configuration Variables
API_KEY = "your_api_key_here"
BASE_URL = "your_base_url_here"
CHAT_MODEL = "your_chat_model_here"
EMBEDDING_MODEL = "your_embedding_model_here"

# Streamlit page configuration
st.set_page_config(page_title='RAG PDF Summarizer Chatbot', layout='wide')

# Tiktoken configuration
tiktoken_cache_dir = "./token"
os.environ["TIKTOKEN_CACHE_DIR"] = tiktoken_cache_dir

# HTTP client
client = httpx.Client(verify=False)

# Initialize session state
if "messages" not in st.session_state:
    # Start with system prompt as LangChain SystemMessage object
    system_prompt = (
        "You are a helpful assistant. If the user wants to summarize a PDF, "
        "prompt them to upload one. Otherwise, answer questions concisely."
    )
    st.session_state.messages = [SystemMessage(content=system_prompt)]

if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None

if "rag_ready" not in st.session_state:
    st.session_state.rag_ready = False

if "expecting_pdf" not in st.session_state:
    st.session_state.expecting_pdf = False

# Helper function to display chat messages
def display_chat(messages: List[Union[HumanMessage, AIMessage, SystemMessage]]):
    st.markdown("### Chat History")
    # Show oldest first (top to bottom)
    for msg in messages:
        if isinstance(msg, HumanMessage):
            st.markdown(
                f"<div style='text-align: left; background-color: #f0f2f6; padding: 12px; border-radius: 10px; margin: 5px 0;'>"
                f"<strong>You:</strong> {msg.content}</div>",
                unsafe_allow_html=True
            )
        elif isinstance(msg, AIMessage):
            st.markdown(
                f"<div style='text-align: right; background-color: #d3f9d8; padding: 12px; border-radius: 10px; margin: 5px 0;'>"
                f"<strong>AI:</strong> {msg.content}</div>",
                unsafe_allow_html=True
            )
        elif isinstance(msg, SystemMessage):
            # Optionally skip system messages or display differently
            pass

# Detect if last user message contains RAG keywords
def last_user_message_contains_keywords(keywords):
    # Find last HumanMessage content
    for msg in reversed(st.session_state.messages):
        if isinstance(msg, HumanMessage):
            content_lower = msg.content.lower()
            return any(k in content_lower for k in keywords)
    return False

# Page title
st.title("ðŸ¤– RAG PDF Summarizer AI Chatbot")

# Main Logic
rag_keywords = ["pdf", "summarize", "summarizer"]

# If expecting pdf and rag not ready, show uploader
if st.session_state.expecting_pdf and not st.session_state.rag_ready:
    st.subheader("ðŸ“„ Upload a PDF file to summarize")
    
    uploaded_file = st.file_uploader("Upload your PDF here", type="pdf")
    
    if uploaded_file:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(uploaded_file.read())
            temp_pdf_path = tmp.name
        
        with st.spinner("Extracting and processing PDF..."):
            # Extract text from PDF
            raw_text = extract_text(temp_pdf_path)
            
            # Split text into chunks
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000, 
                chunk_overlap=200
            )
            chunks = text_splitter.split_text(raw_text)
            
            # Embedding model
            embedding_model = OpenAIEmbeddings(
                base_url=BASE_URL,
                model=EMBEDDING_MODEL,
                api_key=API_KEY,
                http_client=client
            )
            
            # Create vector database
            vectordb = Chroma.from_texts(
                chunks, 
                embedding_model, 
                persist_directory="./chroma_index"
            )
            vectordb.persist()
            
            # LLM model
            llm = ChatOpenAI(
                base_url=BASE_URL,
                model=CHAT_MODEL,
                api_key=API_KEY,
                http_client=client
            )
            
            # Create RAG chain
            retriever = vectordb.as_retriever()
            rag_chain = RetrievalQA.from_chain_type(
                llm=llm, 
                retriever=retriever, 
                return_source_documents=True
            )
            
            st.session_state.rag_chain = rag_chain
            st.session_state.rag_ready = True
            st.success("âœ… PDF processed! You can now ask questions about it below.")
            
            # Clear expecting PDF since processed
            st.session_state.expecting_pdf = False

# Display chat history on top
display_chat(st.session_state.messages)

# Input area
if st.session_state.rag_ready:
    prompt = st.chat_input("Ask a question about the uploaded PDF...")
    if prompt:
        # Save user message
        st.session_state.messages.append(HumanMessage(content=prompt))
        
        with st.spinner("ðŸ” Querying RAG model..."):
            result = st.session_state.rag_chain.invoke(prompt)
            answer = result["result"]
            
            st.session_state.messages.append(AIMessage(content=answer))
            st.rerun()

else:
    prompt = st.chat_input("Ask me anything...")
    if prompt:
        # Save user message
        st.session_state.messages.append(HumanMessage(content=prompt))
        
        # Check if user wants to start PDF summarization
        if any(keyword in prompt.lower() for keyword in rag_keywords):
            st.session_state.expecting_pdf = True
            st.rerun()
        else:
            # Setup Chat LLM
            chat = ChatOpenAI(
                base_url=BASE_URL,
                api_key=API_KEY,
                temperature=0.7,
                model=CHAT_MODEL,
                http_client=client
            )
            
            # Agent State class
            class AgentState(TypedDict):
                messages: List[Union[HumanMessage, AIMessage, SystemMessage]]
            
            # First node function
            def first_node(state: AgentState) -> AgentState:
                response = chat.invoke(state["messages"])
                state["messages"].append(AIMessage(content=response.content))
                return state
            
            # Create and compile graph
            graph = StateGraph(AgentState)
            graph.add_node("node1", first_node)
            graph.add_edge(START, "node1")
            graph.add_edge("node1", END)
            
            agent = graph.compile()
            
            # Convert all messages in session state to LangChain message objects
            state_input = {"messages": st.session_state.messages.copy()}
            
            result = agent.invoke(state_input)
            response = result["messages"][-1].content
            
            # Append AI response to messages
            st.session_state.messages.append(AIMessage(content=response))
            st.rerun()
