from typing import TypedDict, List, Union
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langgraph.graph import StateGraph, START, END
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains.retrieval_qa.base import RetrievalQA
from pdfplumber.high_level import extract_text
import tempfile
import os
import httpx
import tiktoken
import streamlit as st
from datetime import datetime
import json
from io import BytesIO
from pptx import Presentation
from pptx.util import Pt
from fpdf import FPDF
import spacy
import pandas as pd
import pdfplumber
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import base64
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from reportlab.lib.enums import TA_LEFT, TA_CENTER

# Configuration Variables
API_KEY = "your-api-key-here"
BASE_URL = "https://your-base-url-here"

# Available Models List with their parameters
CHAT_MODELS = {
    "gpt-4o": {"max_tokens": 4096, "temperature": (0.0, 2.0), "top_p": (0.0, 1.0)},
    "gpt-4o-mini": {"max_tokens": 16384, "temperature": (0.0, 2.0), "top_p": (0.0, 1.0)},
    "gpt-4-turbo": {"max_tokens": 4096, "temperature": (0.0, 2.0), "top_p": (0.0, 1.0)},
    "gpt-4": {"max_tokens": 8192, "temperature": (0.0, 2.0), "top_p": (0.0, 1.0)},
    "gpt-3.5-turbo": {"max_tokens": 4096, "temperature": (0.0, 2.0), "top_p": (0.0, 1.0)},
    "text-davinci-003": {"max_tokens": 4097, "temperature": (0.0, 2.0), "top_p": (0.0, 1.0)},
    "your-custom-model-1": {"max_tokens": 4096, "temperature": (0.0, 2.0), "top_p": (0.0, 1.0)},
    "your-custom-model-2": {"max_tokens": 4096, "temperature": (0.0, 2.0), "top_p": (0.0, 1.0)}
}

EMBEDDING_MODELS = [
    "text-embedding-ada-002",
    "text-embedding-3-small", 
    "text-embedding-3-large",
    "your-custom-embedding-model"
]

# Configure
st.set_page_config(page_title='Advanced RAG PDF Analyzer', layout='wide')

tiktoken_cache_dir = "./token"
os.environ["TIKTOKEN_CACHE_DIR"] = tiktoken_cache_dir
client = httpx.Client(verify=False)

# Load NER model
@st.cache_resource
def load_ner_model():
    try:
        return spacy.load("en_core_web_sm")
    except IOError:
        st.error("Please install spaCy English model: python -m spacy download en_core_web_sm")
        return None

nlp = load_ner_model()

# Table extraction functions
def extract_tables_from_pdf(pdf_file):
    """Extract tables from PDF using pdfplumber"""
    tables_data = []
    
    with pdfplumber.open(pdf_file) as pdf:
        for page_num, page in enumerate(pdf.pages):
            tables = page.extract_tables()
            if tables:
                for table_idx, table in enumerate(tables):
                    if table and len(table) > 1:  # Ensure table has header and data
                        df = pd.DataFrame(table[1:], columns=table[0])
                        # Clean the dataframe
                        df = df.dropna(how='all').dropna(axis=1, how='all')
                        if not df.empty:
                            table_info = {
                                'page': page_num + 1,
                                'table_index': table_idx + 1,
                                'dataframe': df,
                                'title': f"Table {table_idx + 1} from Page {page_num + 1}"
                            }
                            tables_data.append(table_info)
    
    return tables_data

def format_table_for_llm(df, title):
    """Convert DataFrame to readable format for LLM"""
    formatted_text = f"\n{title}:\n"
    formatted_text += f"Shape: {df.shape[0]} rows Ã— {df.shape[1]} columns\n"
    formatted_text += f"Columns: {', '.join(df.columns.astype(str))}\n\n"
    formatted_text += df.to_string(index=False)
    formatted_text += "\n" + "="*50 + "\n"
    return formatted_text

def analyze_table_with_llm(df, title, chat_model, model_params):
    """Generate analysis of table using LLM"""
    chat = ChatOpenAI(
        base_url=BASE_URL,
        api_key=API_KEY,
        model=chat_model,
        temperature=model_params['temperature'],
        max_tokens=model_params['max_tokens'],
        top_p=model_params.get('top_p', 1.0),
        http_client=client
    )
    
    table_text = format_table_for_llm(df, title)
    
    analysis_prompt = f"""
    Analyze the following table data and provide insights:
    
    {table_text}
    
    Please provide:
    1. Summary of the data
    2. Key insights and patterns
    3. Notable statistics
    4. Data quality observations
    5. Recommendations for further analysis
    """
    
    response = chat.invoke([HumanMessage(content=analysis_prompt)])
    return response.content

# Visualization functions
def create_table_visualizations(df, title):
    """Create various visualizations for table data"""
    visualizations = []
    
    # Only create visualizations for numeric columns
    numeric_columns = df.select_dtypes(include=['number']).columns
    
    if len(numeric_columns) > 0:
        # Bar chart for numeric columns
        if len(numeric_columns) <= 5:  # Limit to prevent overcrowding
            fig = px.bar(df, y=numeric_columns.tolist(), 
                        title=f"{title} - Numeric Data Overview")
            visualizations.append(("bar_chart", fig))
        
        # Correlation heatmap if multiple numeric columns
        if len(numeric_columns) > 1:
            corr_matrix = df[numeric_columns].corr()
            fig = px.imshow(corr_matrix, 
                           title=f"{title} - Correlation Matrix",
                           color_continuous_scale='RdBu_r')
            visualizations.append(("correlation", fig))
        
        # Distribution plots for first few numeric columns
        for i, col in enumerate(numeric_columns[:3]):  # Limit to first 3
            fig = px.histogram(df, x=col, 
                             title=f"{title} - Distribution of {col}")
            visualizations.append((f"distribution_{col}", fig))
    
    return visualizations

# File generation functions (enhanced)
def create_pptx(summary_text: str, filename: str) -> bytes:
    """Create PowerPoint file from text"""
    prs = Presentation()
    
    # Title slide
    title_slide_layout = prs.slide_layouts[0]
    slide = prs.slides.add_slide(title_slide_layout)
    title = slide.shapes.title
    subtitle = slide.placeholders[1]
    title.text = f"Summary: {filename}"
    subtitle.text = "Generated Summary"
    
    # Content slide
    bullet_slide_layout = prs.slide_layouts[1]
    slide = prs.slides.add_slide(bullet_slide_layout)
    shapes = slide.shapes
    title_shape = shapes.title
    body_shape = shapes.placeholders[1]
    
    title_shape.text = "Key Points"
    tf = body_shape.text_frame
    tf.clear()
    
    # Add bullet points
    lines = [line.strip() for line in summary_text.split('\n') if line.strip()]
    for i, line in enumerate(lines[:10]):  # Limit to 10 points
        if i == 0:
            tf.text = line
        else:
            p = tf.add_paragraph()
            p.text = line
            p.level = 0
            p.font.size = Pt(14)
    
    # Save to bytes
    pptx_io = BytesIO()
    prs.save(pptx_io)
    pptx_io.seek(0)
    return pptx_io.read()

def create_pdf(text: str, filename: str) -> bytes:
    """Create PDF file from text"""
    pdf = FPDF()
    pdf.add_page()
    pdf.set_auto_page_break(auto=True, margin=15)
    
    # Title
    pdf.set_font("Arial", 'B', 16)
    pdf.cell(0, 10, f'Summary: {filename}', ln=True, align='C')
    pdf.ln(10)
    
    # Content
    pdf.set_font("Arial", size=12)
    for line in text.split('\n'):
        if line.strip():
            pdf.multi_cell(0, 8, line.strip())
            pdf.ln(2)
    
    pdf_output = BytesIO()
    pdf.output(pdf_output)
    pdf_output.seek(0)
    return pdf_output.read()

def create_chat_session_pdf(messages: List, session_name: str) -> bytes:
    """Create PDF from chat session with emojis"""
    buffer = BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=letter)
    styles = getSampleStyleSheet()
    story = []
    
    # Title
    title_style = ParagraphStyle(
        'CustomTitle',
        parent=styles['Heading1'],
        fontSize=18,
        spaceAfter=30,
        alignment=TA_CENTER
    )
    story.append(Paragraph(f"Chat Session: {session_name}", title_style))
    story.append(Spacer(1, 12))
    
    # Messages
    user_style = ParagraphStyle(
        'UserStyle',
        parent=styles['Normal'],
        fontSize=11,
        leftIndent=20,
        spaceAfter=10,
        alignment=TA_LEFT
    )
    
    bot_style = ParagraphStyle(
        'BotStyle',
        parent=styles['Normal'],
        fontSize=11,
        leftIndent=20,
        spaceAfter=10,
        alignment=TA_LEFT
    )
    
    for msg in messages[1:]:  # Skip system message
        if isinstance(msg, HumanMessage):
            story.append(Paragraph(f"ðŸ‘¤ <b>User:</b> {msg.content}", user_style))
        elif isinstance(msg, AIMessage):
            story.append(Paragraph(f"ðŸ¤– <b>Assistant:</b> {msg.content}", bot_style))
        story.append(Spacer(1, 6))
    
    # Generate timestamp
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    story.append(Spacer(1, 20))
    story.append(Paragraph(f"Generated on: {timestamp}", styles['Normal']))
    
    doc.build(story)
    buffer.seek(0)
    return buffer.read()

def extract_entities(text: str) -> dict:
    """Extract named entities using spaCy NER"""
    if not nlp:
        return {}
    
    doc = nlp(text)
    entities = {}
    
    for ent in doc.ents:
        if ent.label_ not in entities:
            entities[ent.label_] = []
        if ent.text not in entities[ent.label_]:
            entities[ent.label_].append(ent.text)
    
    return entities

def format_entities_for_display(entities: dict) -> str:
    """Format entities for nice display"""
    if not entities:
        return "No entities found."
    
    formatted = "## Named Entities Found:\n\n"
    
    entity_labels = {
        'PERSON': 'ðŸ‘¤ People',
        'ORG': 'ðŸ¢ Organizations', 
        'GPE': 'ðŸŒ Places',
        'MONEY': 'ðŸ’° Money',
        'DATE': 'ðŸ“… Dates',
        'TIME': 'â° Times',
        'PRODUCT': 'ðŸ“¦ Products',
        'EVENT': 'ðŸŽ‰ Events',
        'WORK_OF_ART': 'ðŸŽ¨ Works of Art',
        'LAW': 'âš–ï¸ Laws',
        'LANGUAGE': 'ðŸ—£ï¸ Languages'
    }
    
    for label, items in entities.items():
        display_label = entity_labels.get(label, f'ðŸ“‹ {label}')
        formatted += f"**{display_label}:**\n"
        for item in items[:5]:  # Limit to 5 per category
            formatted += f"- {item}\n"
        formatted += "\n"
    
    return formatted

# Initialize session state
def initialize_session_state():
    if "chat_sessions" not in st.session_state:
        st.session_state.chat_sessions = {}
    
    if "current_session_id" not in st.session_state:
        session_id = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        st.session_state.current_session_id = session_id
        st.session_state.chat_sessions[session_id] = {
            "messages": [SystemMessage(content="You are a helpful assistant.")],
            "rag_chain": None,
            "rag_ready": False,
            "pdf_name": None,
            "session_type": "chat",
            "text_chunks": None,
            "entities": None,
            "tables": None,
            "table_analyses": {}
        }
    
    if "selected_chat_model" not in st.session_state:
        st.session_state.selected_chat_model = list(CHAT_MODELS.keys())[0]
    
    if "selected_embedding_model" not in st.session_state:
        st.session_state.selected_embedding_model = EMBEDDING_MODELS[0]
    
    # Model parameters
    if "model_temperature" not in st.session_state:
        st.session_state.model_temperature = 0.7
    
    if "model_max_tokens" not in st.session_state:
        st.session_state.model_max_tokens = 1000
    
    if "model_top_p" not in st.session_state:
        st.session_state.model_top_p = 1.0

def get_current_session():
    return st.session_state.chat_sessions[st.session_state.current_session_id]

def create_new_session(session_type="chat"):
    session_id = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    st.session_state.chat_sessions[session_id] = {
        "messages": [SystemMessage(content="You are a helpful assistant.")],
        "rag_chain": None,
        "rag_ready": False,
        "pdf_name": None,
        "session_type": session_type,
        "text_chunks": None,
        "entities": None,
        "tables": None,
        "table_analyses": {}
    }
    st.session_state.current_session_id = session_id
    return session_id

def delete_session(session_id):
    if len(st.session_state.chat_sessions) > 1:
        del st.session_state.chat_sessions[session_id]
        if st.session_state.current_session_id == session_id:
            st.session_state.current_session_id = list(st.session_state.chat_sessions.keys())[0]

# Helper to display chat messages
def display_chat(messages: List[Union[HumanMessage, AIMessage, SystemMessage]]):
    for msg in messages[1:]:  # Skip system message
        if isinstance(msg, HumanMessage):
            st.chat_message("user").write(msg.content)
        elif isinstance(msg, AIMessage):
            st.chat_message("assistant").write(msg.content)

# Generate PDF Summary
def generate_pdf_summary(text_chunks):
    model_params = {
        'temperature': st.session_state.model_temperature,
        'max_tokens': st.session_state.model_max_tokens,
        'top_p': st.session_state.model_top_p
    }
    
    chat = ChatOpenAI(
        base_url=BASE_URL,
        api_key=API_KEY,
        model=st.session_state.selected_chat_model,
        temperature=model_params['temperature'],
        max_tokens=model_params['max_tokens'],
        top_p=model_params['top_p'],
        http_client=client
    )
    
    combined_text = "\n".join(text_chunks[:10])  # Limit for summary
    summary_prompt = f"Please provide a comprehensive summary of the following PDF content:\n\n{combined_text}"
    
    response = chat.invoke([HumanMessage(content=summary_prompt)])
    return response.content

# Initialize
initialize_session_state()

# Sidebar
with st.sidebar:
    st.title("ðŸ¤– Advanced AI Chat Assistant")
    
    # Model Selection
    st.subheader("ðŸ› ï¸ Model Settings")
    st.session_state.selected_chat_model = st.selectbox(
        "Chat Model:",
        list(CHAT_MODELS.keys()),
        index=list(CHAT_MODELS.keys()).index(st.session_state.selected_chat_model)
    )
    
    st.session_state.selected_embedding_model = st.selectbox(
        "Embedding Model:",
        EMBEDDING_MODELS,
        index=EMBEDDING_MODELS.index(st.session_state.selected_embedding_model)
    )
    
    # Model Parameters
    st.subheader("âš™ï¸ Model Parameters")
    current_model_config = CHAT_MODELS[st.session_state.selected_chat_model]
    
    st.session_state.model_temperature = st.slider(
        "Temperature:",
        min_value=current_model_config["temperature"][0],
        max_value=current_model_config["temperature"][1],
        value=st.session_state.model_temperature,
        step=0.1,
        help="Controls randomness. Higher values make output more random."
    )
    
    st.session_state.model_max_tokens = st.slider(
        "Max Tokens:",
        min_value=100,
        max_value=current_model_config["max_tokens"],
        value=min(st.session_state.model_max_tokens, current_model_config["max_tokens"]),
        step=100,
        help="Maximum number of tokens to generate."
    )
    
    if "top_p" in current_model_config:
        st.session_state.model_top_p = st.slider(
            "Top P:",
            min_value=current_model_config["top_p"][0],
            max_value=current_model_config["top_p"][1],
            value=st.session_state.model_top_p,
            step=0.1,
            help="Controls diversity. Lower values make output more focused."
        )
    
    st.divider()
    
    # PDF Upload Section
    st.subheader("ðŸ“„ PDF Upload")
    uploaded_file = st.file_uploader("Upload PDF for Analysis", type="pdf")
    
    if uploaded_file:
        if st.button("ðŸš€ Process PDF"):
            # Create new PDF session
            create_new_session("pdf")
            current_session = get_current_session()
            current_session["pdf_name"] = uploaded_file.name
            
            with st.spinner("Processing PDF..."):
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                    tmp.write(uploaded_file.read())
                    temp_pdf_path = tmp.name
                
                # Extract text
                raw_text = extract_text(temp_pdf_path)
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
                chunks = text_splitter.split_text(raw_text)
                
                # Extract tables
                tables_data = extract_tables_from_pdf(temp_pdf_path)
                current_session["tables"] = tables_data
                
                # Extract entities
                entities = extract_entities(raw_text)
                current_session["entities"] = entities
                
                # Embedding model
                embedding_model = OpenAIEmbeddings(
                    base_url=BASE_URL,
                    model=st.session_state.selected_embedding_model,
                    api_key=API_KEY,
                    http_client=client
                )
                
                vectordb = Chroma.from_texts(chunks, embedding_model, persist_directory="./chroma_index")
                vectordb.persist()
                
                # LLM model
                model_params = {
                    'temperature': st.session_state.model_temperature,
                    'max_tokens': st.session_state.model_max_tokens,
                    'top_p': st.session_state.model_top_p
                }
                
                llm = ChatOpenAI(
                    base_url=BASE_URL,
                    model=st.session_state.selected_chat_model,
                    api_key=API_KEY,
                    temperature=model_params['temperature'],
                    max_tokens=model_params['max_tokens'],
                    top_p=model_params['top_p'],
                    http_client=client
                )
                
                retriever = vectordb.as_retriever()
                rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)
                
                current_session["rag_chain"] = rag_chain
                current_session["rag_ready"] = True
                current_session["text_chunks"] = chunks
                
                st.success(f"âœ… PDF processed successfully! Found {len(tables_data)} tables.")
                st.rerun()
    
    # PDF Actions (only show if PDF is loaded)
    current_session = get_current_session()
    if current_session["rag_ready"]:
        st.subheader("ðŸ“Š PDF Actions")
        
        # Download format selection
        download_format = st.selectbox(
            "ðŸ“¥ Download Format:",
            ["txt", "pdf", "pptx"],
            key="download_format"
        )
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ðŸ“„ Generate Summary"):
                with st.spinner("Generating summary..."):
                    summary = generate_pdf_summary(current_session["text_chunks"])
                    filename = current_session["pdf_name"].replace(".pdf", "")
                    
                    if download_format == "txt":
                        st.download_button(
                            label="ðŸ’¾ Download TXT",
                            data=summary,
                            file_name=f"{filename}_summary.txt",
                            mime="text/plain"
                        )
                    elif download_format == "pdf":
                        pdf_data = create_pdf(summary, filename)
                        st.download_button(
                            label="ðŸ’¾ Download PDF",
                            data=pdf_data,
                            file_name=f"{filename}_summary.pdf",
                            mime="application/pdf"
                        )
                    elif download_format == "pptx":
                        pptx_data = create_pptx(summary, filename)
                        st.download_button(
                            label="ðŸ’¾ Download PPTX",
                            data=pptx_data,
                            file_name=f"{filename}_summary.pptx",
                            mime="application/vnd.openxmlformats-officedocument.presentationml.presentation"
                        )
        
        with col2:
            if st.button("ðŸ” Show NER"):
                if current_session["entities"]:
                    st.markdown("### Named Entities")
                    entity_display = format_entities_for_display(current_session["entities"])
                    st.markdown(entity_display)
        
        # Table Actions
        if current_session["tables"]:
            st.subheader("ðŸ“‹ Table Analysis")
            
            # Table selection
            table_options = [f"{table['title']}" for table in current_session["tables"]]
            selected_table = st.selectbox("Select Table:", table_options)
            
            if selected_table:
                table_idx = table_options.index(selected_table)
                selected_table_data = current_session["tables"][table_idx]
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    if st.button("ðŸ“Š Analyze Table"):
                        with st.spinner("Analyzing table..."):
                            model_params = {
                                'temperature': st.session_state.model_temperature,
                                'max_tokens': st.session_state.model_max_tokens,
                                'top_p': st.session_state.model_top_p
                            }
                            analysis = analyze_table_with_llm(
                                selected_table_data['dataframe'], 
                                selected_table_data['title'],
                                st.session_state.selected_chat_model,
                                model_params
                            )
                            current_session["table_analyses"][selected_table] = analysis
                            st.success("Analysis completed!")
                
                with col2:
                    if st.button("ðŸ’¾ Download CSV"):
                        csv = selected_table_data['dataframe'].to_csv(index=False)
                        st.download_button(
                            label="Download CSV",
                            data=csv,
                            file_name=f"{selected_table.replace(' ', '_')}.csv",
                            mime="text/csv"
                        )
                
                with col3:
                    if st.button("ðŸ“ˆ Visualize"):
                        st.session_state.show_visualizations = True
    
    st.divider()
    
    # Session Management
    st.subheader("ðŸ’¬ Chat Sessions")
    
    # New Chat Button
    if st.button("âž• New Chat"):
        create_new_session("chat")
        st.rerun()
    
    # Display sessions
    for session_id in list(st.session_state.chat_sessions.keys()):
        session = st.session_state.chat_sessions[session_id]
        session_type = "ðŸ“„ PDF" if session["session_type"] == "pdf" else "ðŸ’¬ Chat"
        session_name = f"{session_type} - {session_id}"
        
        if session["pdf_name"]:
            session_name = f"ðŸ“„ {session['pdf_name'][:15]}... - {session_id[-8:]}"
        
        col1, col2, col3 = st.columns([2, 1, 1])
        
        with col1:
            if st.button(
                session_name,
                key=f"session_{session_id}",
                type="primary" if session_id == st.session_state.current_session_id else "secondary"
            ):
                st.session_state.current_session_id = session_id
                st.rerun()
        
        with col2:
            # Download chat session button
            if st.button("ðŸ“¥", key=f"download_{session_id}", help="Download chat session as PDF"):
                session_data = st.session_state.chat_sessions[session_id]
                pdf_data = create_chat_session_pdf(session_data["messages"], session_name)
                st.download_button(
                    label="Download",
                    data=pdf_data,
                    file_name=f"chat_session_{session_id[-8:]}.pdf",
                    mime="application/pdf",
                    key=f"download_btn_{session_id}"
                )
        
        with col3:
            if st.button("ðŸ—‘ï¸", key=f"delete_{session_id}"):
                delete_session(session_id)
                st.rerun()
    
    # Clear All Chats
    if st.button("ðŸ§¹ Clear All Chats"):
        st.session_state.chat_sessions = {}
        create_new_session("chat")
        st.rerun()

# Main Chat Interface
current_session = get_current_session()
session_type_icon = "ðŸ“„" if current_session["session_type"] == "pdf" else "ðŸ’¬"

if current_session["pdf_name"]:
    st.title(f"{session_type_icon} PDF Analysis - {current_session['pdf_name']}")
else:
    st.title(f"{session_type_icon} AI Chat Assistant")

# Display current model info and parameters
st.caption(f"Model: {st.session_state.selected_chat_model} | Temp: {st.session_state.model_temperature} | Max Tokens: {st.session_state.model_max_tokens} | Session: {st.session_state.current_session_id[-8:]}")

# Display tables if available
if current_session.get("tables"):
    with st.expander(f"ðŸ“‹ Extracted Tables ({len(current_session['tables'])} found)", expanded=False):
        for i, table_data in enumerate(current_session["tables"]):
            st.subheader(table_data["title"])
            st.dataframe(table_data["dataframe"], use_container_width=True)
            
            # Show analysis if available
            if table_data["title"] in current_session["table_analyses"]:
                with st.expander(f"Analysis for {table_data['title']}", expanded=False):
                    st.write(current_session["table_analyses"][table_data["title"]])

# Display visualizations if requested
if hasattr(st.session_state, 'show_visualizations') and st.session_state.show_visualizations:
    if current_session.get("tables"):
        st.subheader("ðŸ“ˆ Data Visualizations")
        
        # Get the selected table from the sidebar
        table_options = [f"{table['title']}" for table in current_session["tables"]]
        if table_options:
            # Use the first table or the one selected in sidebar
            selected_idx = 0  # You can make this dynamic based on sidebar selection
            table_data = current_session["tables"][selected_idx]
            
            visualizations = create_table_visualizations(
                table_data["dataframe"], 
                table_data["title"]
            )
            
            if visualizations:
                for viz_name, fig in visualizations:
                    st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("No suitable numeric data found for visualization.")
        
        # Reset the visualization flag
        st.session_state.show_visualizations = False

# Display NER results if available
if current_session.get("entities"):
    with st.expander("ðŸ” Named Entities Found", expanded=False):
        entity_display = format_entities_for_display(current_session["entities"])
        st.markdown(entity_display)

# Display chat messages
display_chat(current_session["messages"])

# Chat input
if current_session["rag_ready"]:
    prompt = st.chat_input("Ask a question about the uploaded PDF or its tables...")
    if prompt:
        # Add user message
        current_session["messages"].append(HumanMessage(content=prompt))
        st.chat_message("user").write(prompt)
        
        # Generate response
        with st.spinner("ðŸ¤” Thinking..."):
            # Add table context if available
            context_prompt = prompt
            if current_session["tables"]:
                table_context = "\n\nAvailable Tables:\n"
                for table_data in current_session["tables"]:
                    table_context += format_table_for_llm(table_data["dataframe"], table_data["title"])
                context_prompt = prompt + table_context
            
            result = current_session["rag_chain"].invoke(context_prompt)
            answer = result["result"]
            current_session["messages"].append(AIMessage(content=answer))
            st.chat_message("assistant").write(answer)
            st.rerun()

else:
    prompt = st.chat_input("Ask me anything...")
    if prompt:
        # Add user message
        current_session["messages"].append(HumanMessage(content=prompt))
        st.chat_message("user").write(prompt)
        
        # Generate response
        with st.spinner("ðŸ¤” Thinking..."):
            model_params = {
                'temperature': st.session_state.model_temperature,
                'max_tokens': st.session_state.model_max_tokens,
                'top_p': st.session_state.model_top_p
            }
            
            chat = ChatOpenAI(
                base_url=BASE_URL,
                api_key=API_KEY,
                temperature=model_params['temperature'],
                max_tokens=model_params['max_tokens'],
                top_p=model_params['top_p'],
                model=st.session_state.selected_chat_model,
                http_client=client
            )

            class AgentState(TypedDict):
                messages: List[Union[HumanMessage, AIMessage, SystemMessage]]

            def first_node(state: AgentState) -> AgentState:
                response = chat.invoke(state["messages"])
                state["messages"].append(AIMessage(content=response.content))
                return state

            graph = StateGraph(AgentState)
            graph.add_node("node1", first_node)
            graph.add_edge(START, "node1")
            graph.add_edge("node1", END)
            agent = graph.compile()

            state_input = {"messages": current_session["messages"].copy()}
            result = agent.invoke(state_input)
            response = result["messages"][-1].content

            current_session["messages"].append(AIMessage(content=response))
            st.chat_message("assistant").write(response)
            st.rerun()

# Footer with additional info
st.divider()
st.caption("ðŸ’¡ Tips: Upload PDFs to extract tables, analyze data, and create visualizations. Use model parameters to fine-tune responses.")
