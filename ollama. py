import streamlit as st
from pdfminer.high_level import extract_text
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
import tempfile
import httpx

st.set_page_config(page_title="RAG PDF Chatbot", layout="wide")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None

with st.sidebar:
    st.header("üìÇ Upload Document")
    uploaded_file = st.file_uploader("Upload a PDF", type="pdf")
    if uploaded_file:
        st.success(f"‚úÖ {uploaded_file.name} uploaded successfully!")
    start_chat = st.button("üöÄ Start Chat")
    clear_history = st.button("üóëÔ∏è Clear Chat History")
    st.divider()
    st.subheader("üí¨ Conversation History")
    for idx, (q, a) in enumerate(st.session_state.chat_history):
        with st.expander(f"Q{idx+1}: {q[:30]}..."):
            st.write(a)

if clear_history:
    st.session_state.chat_history = []

client = httpx.Client(verify=False)

llm = ChatOpenAI(
    api_key="YOUR_API_KEY",
    base_url="YOUR_BASE_URL",
    model="YOUR_LLM_MODEL",
    http_client=client
)

embedding_model = OpenAIEmbeddings(
    api_key="YOUR_API_KEY",
    base_url="YOUR_BASE_URL",
    model="YOUR_EMBED_MODEL",
    http_client=client
)

if uploaded_file and start_chat:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
        temp_file.write(uploaded_file.read())
        temp_file_path = temp_file.name

    raw_text = extract_text(temp_file_path)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_text(raw_text)

    with st.spinner("Indexing document..."):
        vectordb = Chroma.from_texts(chunks, embedding_model, persist_directory="chroma_index")
        vectordb.persist()
        retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 5})
        st.session_state.rag_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            return_source_documents=True
        )

st.title("ü§ñ Continuous RAG-powered PDF Chatbot")

if st.session_state.rag_chain:
    user_input = st.chat_input("Ask something...")
    if user_input:
        with st.spinner("Bot is thinking..."):
            result = st.session_state.rag_chain.invoke(
                {"question": user_input, "chat_history": st.session_state.chat_history}
            )
            answer = result["answer"]
            st.session_state.chat_history.append((user_input, answer))

    for q, a in st.session_state.chat_history:
        with st.chat_message("user"):
            st.write(q)
        with st.chat_message("assistant"):
            st.write(a)