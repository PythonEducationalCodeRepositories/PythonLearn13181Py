from typing import TypedDict, List, Union
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langgraph.graph import StateGraph, START, END
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains.retrieval_qa.base import RetrievalQA
from pdfplumber.high_level import extract_text
import tempfile
import os
import httpx
import tiktoken
import streamlit as st

# Configuration Variables
API_KEY = "your-api-key-here"
BASE_URL = "https://your-base-url-here"
CHAT_MODEL = "your-chat-model-name"
EMBEDDING_MODEL = "your-embedding-model-name"

# Configure
st.set_page_config(page_title='RAG PDF Summarizer Chatbot', layout='wide')

tiktoken_cache_dir = "./token"
os.environ["TIKTOKEN_CACHE_DIR"] = tiktoken_cache_dir
client = httpx.Client(verify=False)

# Initialize session state
if "messages" not in st.session_state:
    # Start with system prompt as LangChain SystemMessage object
    system_prompt = (
        "You are a helpful assistant. If the user wants to summarize a PDF, "
        "prompt them to upload one. Otherwise, answer questions concisely."
    )
    st.session_state.messages = [SystemMessage(content=system_prompt)]

if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None

if "rag_ready" not in st.session_state:
    st.session_state.rag_ready = False

if "expecting_pdf" not in st.session_state:
    st.session_state.expecting_pdf = False

# Helper to display chat messages
def display_chat(messages: List[Union[HumanMessage, AIMessage, SystemMessage]]):
    st.markdown("### Chat History")
    # Show oldest first (top to bottom)
    for msg in messages:
        if isinstance(msg, HumanMessage):
            st.markdown(
                f"<div style='text-align: left; background-color: #f0f2f6; padding: 12px; border-radius: 10px; margin: 5px 0;'>"
                f"<strong>You:</strong> {msg.content}</div>",
                unsafe_allow_html=True
            )
        elif isinstance(msg, AIMessage):
            st.markdown(
                f"<div style='text-align: right; background-color: #d3f9d8; padding: 12px; border-radius: 10px; margin: 5px 0;'>"
                f"<strong>AI:</strong> {msg.content}</div>",
                unsafe_allow_html=True
            )
        elif isinstance(msg, SystemMessage):
            # Optionally skip system messages or display differently
            pass

# Detect if last user message contains RAG keywords
def last_user_message_contains_keywords(keywords):
    # Find last HumanMessage content
    for msg in reversed(st.session_state.messages):
        if isinstance(msg, HumanMessage):
            content_lower = msg.content.lower()
            return any(k in content_lower for k in keywords)
    return False

# Page title
st.title("🤖 RAG PDF Summarizer AI Chatbot")

# Main Logic
rag_keywords = ["pdf", "summarize", "summarizer"]

# If expecting pdf and rag not ready, show uploader
if st.session_state.expecting_pdf and not st.session_state.rag_ready:
    st.subheader("📄 Upload a PDF file to summarize")
    
    uploaded_file = st.file_uploader("Upload your PDF here", type="pdf")
    
    if uploaded_file:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(uploaded_file.read())
            temp_pdf_path = tmp.name
        
        with st.spinner("Extracting and processing PDF..."):
            raw_text = extract_text(temp_pdf_path)
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = text_splitter.split_text(raw_text)
            
            # Embedding model
            embedding_model = OpenAIEmbeddings(
                base_url=BASE_URL,
                model=EMBEDDING_MODEL,
                api_key=API_KEY,
                http_client=client
            )
            
            vectordb = Chroma.from_texts(chunks, embedding_model, persist_directory="./chroma_index")
            vectordb.persist()
            
            # LLM model
            llm = ChatOpenAI(
                base_url=BASE_URL,
                model=CHAT_MODEL,
                api_key=API_KEY,
                http_client=client
            )
            
            retriever = vectordb.as_retriever()
            rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)
            
            st.session_state.rag_chain = rag_chain
            st.session_state.rag_ready = True
            st.success("✅ PDF processed! You can now ask questions about it below.")
            
            # Clear expecting PDF since processed
            st.session_state.expecting_pdf = False

# Display chat history on top
display_chat(st.session_state.messages)

# Input area
if st.session_state.rag_ready:
    prompt = st.chat_input("Ask a question about the uploaded PDF...")
    if prompt:
        # Save user message
        st.session_state.messages.append(HumanMessage(content=prompt))
        
        with st.spinner("🔍 Querying RAG model..."):
            result = st.session_state.rag_chain.invoke(prompt)
            answer = result["result"]
            st.session_state.messages.append(AIMessage(content=answer))
            st.rerun()

else:
    prompt = st.chat_input("Ask me anything...")
    if prompt:
        # Save user message
        st.session_state.messages.append(HumanMessage(content=prompt))
        
        # Check if user wants to start PDF summarization
        if any(keyword in prompt.lower() for keyword in rag_keywords):
            st.session_state.expecting_pdf = True
            st.rerun()
        else:
            # Setup Chat LLM
            chat = ChatOpenAI(
                base_url=BASE_URL,
                api_key=API_KEY,
                temperature=0.7,
                model=CHAT_MODEL,
                http_client=client
            )

            class AgentState(TypedDict):
                messages: List[Union[HumanMessage, AIMessage, SystemMessage]]

            def first_node(state: AgentState) -> AgentState:
                response = chat.invoke(state["messages"])
                state["messages"].append(AIMessage(content=response.content))
                return state

            graph = StateGraph(AgentState)
            graph.add_node("node1", first_node)
            graph.add_edge(START, "node1")
            graph.add_edge("node1", END)
            agent = graph.compile()

            # Convert all messages in session state to LangChain message objects (they already are)
            state_input = {"messages": st.session_state.messages.copy()}
            result = agent.invoke(state_input)
            response = result["messages"][-1].content

            # Append AI response to messages
            st.session_state.messages.append(AIMessage(content=response))
            st.rerun()
