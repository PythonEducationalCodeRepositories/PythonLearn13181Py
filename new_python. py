file_structure = """
Project Structure:
==================
incident_categorization/
│
├── db_config.py          # Database and embedding configuration
├── faiss_indexing.py     # FAISS index creation and management
├── genai_categorization.py  # AI categorization logic
└── main.py               # Main application orchestration
"""

print(file_structure)
print("\n" + "="*60)
print("Creating modularized files...")
print("="*60)











# FILE 1: db_config.py
# Database and ChromaDB configuration

db_config_content = '''import pandas as pd
import chromadb
from chromadb.utils import embedding_functions


# ============================================
# CONFIGURATION
# ============================================
CSV_FILE = "incidents.csv"


class DatabaseConfig:
    """Handles CSV loading, ChromaDB initialization, and embedding function"""
    
    def __init__(self, csv_file=CSV_FILE):
        self.csv_file = csv_file
        self.df = None
        self.category_col = None
        self.embedding_function = None
        self.collection = None
        self.chroma_client = None
        
    def load_csv(self):
        """Load incident data from CSV"""
        print("\\n[STEP 1] Loading incident data from CSV...")
        try:
            self.df = pd.read_csv(self.csv_file)
            print(f"✅ Successfully loaded {len(self.df)} incidents")
            print(f"Columns: {list(self.df.columns)}")
            print(f"\\nFirst few records:")
            print(self.df.head())
        except Exception as e:
            print(f"❌ Error loading CSV: {e}")
            exit()
        
        # Validate required columns
        if 'Description' not in self.df.columns:
            print("❌ 'Description' column not found in CSV")
            exit()
        
        # Find category/tag column
        for col in ['Category', 'Tag', 'Type', 'Department', 'category', 'tag', 'type']:
            if col in self.df.columns:
                self.category_col = col
                break
        
        if self.category_col:
            print(f"✅ Found category column: '{self.category_col}'")
        else:
            print("❌ No category/tag column found in CSV")
            exit()
        
        return self.df, self.category_col
    
    def initialize_embedding_function(self):
        """Initialize ChromaDB embedding function"""
        print("\\n[STEP 2] Initializing ChromaDB Embedding Function...")
        try:
            self.embedding_function = embedding_functions.DefaultEmbeddingFunction()
            print("✅ ChromaDB DefaultEmbeddingFunction initialized")
        except Exception as e:
            print(f"❌ Error initializing embedding function: {e}")
            exit()
        
        return self.embedding_function
    
    def generate_embeddings(self):
        """Generate embeddings for all descriptions"""
        print("\\n[STEP 3] Generating embeddings for all incident descriptions...")
        try:
            descriptions = self.df['Description'].fillna("").tolist()
            embeddings = self.embedding_function(descriptions)
            
            print(f"✅ Generated {len(embeddings)} embeddings")
            print(f"Embedding dimension: {len(embeddings[0])}")
            
            return embeddings, descriptions
        except Exception as e:
            print(f"❌ Error generating embeddings: {e}")
            exit()
    
    def setup_chromadb_collection(self, descriptions):
        """Create and populate ChromaDB collection"""
        print("\\n[STEP 5] Storing data in ChromaDB collection...")
        try:
            self.chroma_client = chromadb.Client()
            
            # Delete existing collection if exists
            try:
                self.chroma_client.delete_collection(name="incidents")
            except:
                pass
            
            # Create new collection
            self.collection = self.chroma_client.create_collection(
                name="incidents",
                embedding_function=self.embedding_function,
                metadata={"hnsw:space": "cosine"}
            )
            
            # Prepare metadata
            ids = [str(i) for i in range(len(self.df))]
            metadatas = []
            
            for idx, row in self.df.iterrows():
                metadata = {
                    "incident_id": str(row.get('IncidentID', idx)),
                    "date": str(row.get('Date', '')),
                    "category": str(row[self.category_col])
                }
                metadatas.append(metadata)
            
            # Add to collection
            self.collection.add(
                documents=descriptions,
                metadatas=metadatas,
                ids=ids
            )
            
            print(f"✅ Stored {len(descriptions)} incidents in ChromaDB")
            return self.collection
            
        except Exception as e:
            print(f"⚠️ ChromaDB storage warning: {e}")
            return None
    
    def get_data(self):
        """Return dataframe and category column"""
        return self.df, self.category_col
'''

print("\n✅ FILE 1: db_config.py")
print("="*60)
print(db_config_content[:500] + "...\n")

























# FILE 2: faiss_indexing.py
# FAISS index creation and vector search

faiss_indexing_content = '''import numpy as np
import faiss


# ============================================
# CONFIGURATION
# ============================================
SIMILARITY_THRESHOLD = 0.96  # 96% threshold
TOP_K = 10  # Top 10 similar incidents


class FAISSIndexer:
    """Handles FAISS index creation and vector search"""
    
    def __init__(self, similarity_threshold=SIMILARITY_THRESHOLD, top_k=TOP_K):
        self.index = None
        self.embeddings_array = None
        self.similarity_threshold = similarity_threshold
        self.top_k = top_k
        
    def create_index(self, embeddings):
        """Create FAISS index from embeddings"""
        print("\\n[STEP 4] Creating FAISS index for vector search...")
        try:
            # Convert to numpy array
            self.embeddings_array = np.array(embeddings).astype('float32')
            
            # Normalize for cosine similarity
            faiss.normalize_L2(self.embeddings_array)
            
            # Create FAISS index
            dimension = self.embeddings_array.shape[1]
            self.index = faiss.IndexFlatIP(dimension)
            self.index.add(self.embeddings_array)
            
            print(f"✅ FAISS index created with {self.index.ntotal} vectors")
            print(f"Index type: IndexFlatIP (Cosine Similarity)")
            
            return self.index
            
        except Exception as e:
            print(f"❌ Error creating FAISS index: {e}")
            exit()
    
    def search_similar(self, query_embedding):
        """Search for similar incidents using FAISS"""
        try:
            # Convert to numpy array and normalize
            query_array = np.array([query_embedding]).astype('float32')
            faiss.normalize_L2(query_array)
            
            # Search
            scores, indices = self.index.search(query_array, self.top_k)
            scores = scores[0]
            indices = indices[0]
            
            return scores, indices
            
        except Exception as e:
            print(f"❌ Error during search: {e}")
            return None, None
    
    def get_threshold(self):
        """Get similarity threshold"""
        return self.similarity_threshold
    
    def get_top_k(self):
        """Get top K value"""
        return self.top_k


def save_faiss_index(indexer, filepath="faiss_index.bin"):
    """Save FAISS index to disk (optional - for production reuse)"""
    try:
        faiss.write_index(indexer.index, filepath)
        print(f"✅ FAISS index saved to {filepath}")
    except Exception as e:
        print(f"⚠️ Error saving FAISS index: {e}")


def load_faiss_index(filepath="faiss_index.bin"):
    """Load FAISS index from disk (optional - for production reuse)"""
    try:
        index = faiss.read_index(filepath)
        print(f"✅ FAISS index loaded from {filepath}")
        return index
    except Exception as e:
        print(f"⚠️ Error loading FAISS index: {e}")
        return None
'''

print("✅ FILE 2: faiss_indexing.py")
print("="*60)
print(faiss_indexing_content[:500] + "...\n")



























# FILE 3: genai_categorization.py
# AI categorization logic

genai_categorization_content = '''def ai_categorization(description):
    """
    AI-based categorization function
    TODO: Add your AI model here (OpenAI, Ollama, Azure OpenAI, etc.)
    
    Args:
        description (str): Incident description text
    
    Returns:
        str: Assigned category/tag
    
    Example implementation with OpenAI:
    ------------------------------------
    from openai import OpenAI
    
    client = OpenAI(api_key="your-api-key")
    
    prompt = f"""
    You are an incident categorization AI. Analyze the following incident 
    description and assign the most appropriate category.
    
    Incident: {description}
    
    Return only the category name, nothing else.
    """
    
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    
    return response.choices[0].message.content.strip()
    """
    
    print("\\n[AI] Calling AI categorization function...")
    print("⚠️ AI categorization not implemented yet (placeholder)")
    
    # Placeholder - will be implemented later
    # Add your AI model integration here:
    # - OpenAI GPT-4
    # - Azure OpenAI
    # - Ollama (local models)
    # - Google Gemini
    # - Anthropic Claude
    # etc.
    
    # For now, return a generic category
    return "Uncategorized"


# ============================================
# OPTIONAL: Advanced AI Categorization Functions
# ============================================

def ai_categorization_with_context(description, similar_incidents):
    """
    AI categorization with context from similar incidents
    
    Args:
        description (str): New incident description
        similar_incidents (list): List of similar incidents with tags
    
    Returns:
        str: Assigned category/tag
    """
    print("\\n[AI] Calling AI with context from similar incidents...")
    
    # Build context from similar incidents
    context = "\\n".join([
        f"- {inc['description'][:100]} (Tag: {inc['tag']})"
        for inc in similar_incidents[:3]
    ])
    
    # TODO: Send to your AI model with context
    # prompt = f"""
    # Here are some similar incidents:
    # {context}
    # 
    # New incident: {description}
    # 
    # Based on the similar incidents, what category should this be assigned to?
    # """
    
    return "Uncategorized"


def ai_categorization_batch(descriptions):
    """
    Batch AI categorization for multiple incidents
    
    Args:
        descriptions (list): List of incident descriptions
    
    Returns:
        list: List of assigned categories
    """
    print(f"\\n[AI] Batch categorizing {len(descriptions)} incidents...")
    
    # TODO: Implement batch processing with your AI model
    # This is more efficient for processing multiple incidents
    
    return ["Uncategorized"] * len(descriptions)
'''

print("✅ FILE 3: genai_categorization.py")
print("="*60)
print(genai_categorization_content[:500] + "...\n")



























# FILE 4: main.py
# Main application orchestration

main_content = '''import numpy as np
from db_config import DatabaseConfig
from faiss_indexing import FAISSIndexer
from genai_categorization import ai_categorization


print("="*60)
print("🚀 INCIDENT CATEGORIZATION SYSTEM")
print("Using ChromaDB Embeddings + FAISS Vector Search")
print("="*60)


# ============================================
# INITIALIZATION (Run once - save index for production)
# ============================================
def initialize_system():
    """Initialize database, embeddings, and FAISS index"""
    
    # Step 1: Initialize database config
    db_config = DatabaseConfig(csv_file="incidents.csv")
    
    # Step 2: Load CSV data
    df, category_col = db_config.load_csv()
    
    # Step 3: Initialize embedding function
    embedding_function = db_config.initialize_embedding_function()
    
    # Step 4: Generate embeddings
    embeddings, descriptions = db_config.generate_embeddings()
    
    # Step 5: Create FAISS index
    faiss_indexer = FAISSIndexer(similarity_threshold=0.96, top_k=10)
    faiss_indexer.create_index(embeddings)
    
    # Step 6: Setup ChromaDB collection (optional)
    db_config.setup_chromadb_collection(descriptions)
    
    return db_config, faiss_indexer


# ============================================
# MAIN FUNCTION: PROCESS NEW INCIDENT
# ============================================
def process_new_incident(new_description, db_config, faiss_indexer):
    """Process new incident and assign category using vector search"""
    
    print("\\n" + "="*60)
    print("🔍 PROCESSING NEW INCIDENT")
    print("="*60)
    print(f"Description: {new_description}")
    
    # Get data
    df, category_col = db_config.get_data()
    embedding_function = db_config.embedding_function
    
    # Generate embedding for new incident
    print("\\n[VECTOR] Generating embedding for new incident...")
    try:
        new_embedding = embedding_function([new_description])[0]
        print("✅ Embedding generated")
    except Exception as e:
        print(f"❌ Error generating embedding: {e}")
        return None
    
    # FAISS vector search
    print(f"\\n[SEARCH] Finding top {faiss_indexer.get_top_k()} similar incidents using FAISS...")
    scores, indices = faiss_indexer.search_similar(new_embedding)
    
    if scores is None or indices is None:
        print("❌ Search failed")
        return None
    
    print(f"✅ Found {len(indices)} similar incidents")
    print("\\n📊 TOP 10 SIMILAR INCIDENTS:")
    print("-" * 60)
    
    similar_incidents = []
    for i, (idx, score) in enumerate(zip(indices, scores)):
        similarity_percent = score * 100
        incident = df.iloc[idx]
        
        # Get the actual category/tag
        category_tag = str(incident[category_col])
        
        similar_incidents.append({
            'rank': i + 1,
            'index': int(idx),
            'similarity': float(similarity_percent),
            'description': str(incident['Description']),
            'tag': category_tag
        })
        
        print(f"{i+1}. Similarity: {similarity_percent:.2f}%")
        print(f"   Tag: {category_tag}")
        print(f"   Description: {incident['Description'][:70]}...")
        print()
    
    # Calculate average similarity
    avg_similarity = np.mean(scores) * 100
    print(f"\\n[ANALYSIS] Average Similarity Score: {avg_similarity:.2f}%")
    print(f"Threshold: {faiss_indexer.get_threshold() * 100}%")
    
    # Decision based on threshold
    print("\\n[DECISION] Making categorization decision...")
    
    if avg_similarity >= (faiss_indexer.get_threshold() * 100):
        # Use tag from most similar incident
        top_incident = similar_incidents[0]
        assigned_tag = top_incident['tag']
        
        print(f"✅ Average similarity ({avg_similarity:.2f}%) >= Threshold ({faiss_indexer.get_threshold() * 100}%)")
        print(f"📌 Assigning tag from MOST similar incident: '{assigned_tag}'")
        print(f"   (Top match had {top_incident['similarity']:.2f}% similarity)")
        method = "Existing Tag (Vector Search)"
        
    else:
        # Call AI categorization function
        print(f"⚠️ Average similarity ({avg_similarity:.2f}%) < Threshold ({faiss_indexer.get_threshold() * 100}%)")
        print("🤖 Calling AI categorization function...")
        
        assigned_tag = ai_categorization(new_description)
        print(f"📌 Assigned tag from AI: '{assigned_tag}'")
        method = "AI Generated"
    
    # Final result
    print("\\n" + "="*60)
    print("✨ FINAL RESULT")
    print("="*60)
    print(f"New Incident: {new_description}")
    print(f"Assigned Tag: {assigned_tag}")
    print(f"Average Similarity: {avg_similarity:.2f}%")
    print(f"Method: {method}")
    print("="*60)
    
    return {
        'description': new_description,
        'assigned_tag': assigned_tag,
        'avg_similarity': avg_similarity,
        'similar_incidents': similar_incidents,
        'method': method
    }


# ============================================
# MAIN EXECUTION
# ============================================
if __name__ == "__main__":
    # Initialize system (run once - save for production)
    db_config, faiss_indexer = initialize_system()
    
    # User input loop
    print("\\n\\n" + "="*60)
    print("🎯 READY FOR NEW INCIDENT INPUT")
    print("="*60)
    
    print("\\nEnter new incident description (or 'quit' to exit):")
    while True:
        user_input = input("\\n>>> ").strip()
        
        if user_input.lower() in ['quit', 'exit', 'q']:
            print("\\n👋 Exiting system. Goodbye!")
            break
        
        if not user_input:
            print("⚠️ Please enter a valid description")
            continue
        
        # Process the incident
        result = process_new_incident(user_input, db_config, faiss_indexer)
        
        print("\\n" + "-"*60)
        print("Enter another incident description (or 'quit' to exit):")
'''

print("✅ FILE 4: main.py")
print("="*60)
print(main_content[:500] + "...\n")


